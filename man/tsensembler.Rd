% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classdef-tsensembler.R
\name{tsensembler}
\alias{tsensembler}
\title{Time Series Ensemble}
\usage{
tsensembler(timeseries, learner = NULL, learner.pars = NULL,
  varying.embed = FALSE, varying.trainwindow = FALSE,
  committee.ratio = 0.1, embedding.dimension = 40, ma.N = 50,
  aggregationFUN = "emase-wcommittee", verbose = FALSE)
}
\arguments{
\item{timeseries}{A time series of class xts}

\item{learner}{base models to learn the data}

\item{learner.pars}{list with the parameter setting of \code{learner}}

\item{varying.embed}{Logical. If TRUE, each learning model in \code{learner}
is trained using three different subsets of the data w.r.t. embedding dimension.}

\item{varying.trainwindow}{Logical. If TRUE, each learning model in \code{learner}
is trained using three different subsets of the data w.r.t. training window.}

\item{committee.ratio}{Double between 0 and 1 representing the ratio of
learners that should be considered at each prediction time.}

\item{embedding.dimension}{The maximum embedding dimension used to transform the series.
If \code{timeseries} is a single time series and \code{embedding.dimension} is \emph{NULL}
the parameter will be estimated using false nearest neighbors method.}

\item{ma.N}{The number of periods to average over the Squared Error when
computing \strong{MASE} (\emph{Moving Average Squared Error}). This parameter is only
used when the function applied to weight the base models is based on
MASE. The rationale behind this simple heuristic is that base models are weighted according
to their recent performance. Here, recent performance is formalized and quantified as
inversely proportional to the models' MASE. The dynamics of the moving average
yield a flexibility to the combined model, in the sense that it is self-adaptable when
concept drift occurs. Particularly, this parameter \code{ma.N} controls
the reactiveness of the learning system to such events.
A small value of P leads to greater reactiveness, but also makes the ensemble
susceptible to be fooled by outliers. Conversely, higher values of \code{ma.N}
lead to greater stability, while sacrificing some responsiveness. This trade-off
is known in the literature as the stability-plasticity dilemma (Carpenter et al., 1991).}

\item{aggregationFUN}{The function name used to combine the base learners. See \link{combinePredictions}
for a comprehensive explanation.}

\item{verbose}{Logical. If TRUE, information about the learning procedure status is printed into the console.}
}
\description{
\code{tsensembler} identifies the model specs for creating the dynamic
heterogeneous ensemble for time series forecasting.
}
\seealso{
\code{\link{tsesearch}}
}
